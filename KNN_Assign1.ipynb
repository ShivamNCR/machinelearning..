{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37374b2-a535-451e-b05a-af4dd876a9f9",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5772b0b-138b-4571-97dc-a625833e241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69302f8c-0018-4b10-b14a-15ac87405f65",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a7120-a9f4-41d0-9547-00eb529a5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol1...\n",
    "\n",
    "KNN is one of the most basic yet essential classification algorithms in machine learning. It \n",
    "belongs to the supervised learning domain and finds intense application in pattern recognition,\n",
    "data mining, and intrusion detection.\n",
    "\n",
    "The K-NN algorithm works by finding the K nearest neighbors to a given data point based on a\n",
    "distance metric, such as Euclidean distance. The class or value of the data point is then \n",
    "determined by the majority vote or average of the K neighbors.\n",
    "\n",
    "This approach allows the algorithm to adapt to different patterns and make predictions based \n",
    "on the local structure of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc22528-9168-475b-a446-d852f6b653ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "Cross-validation methods can help in selecting the best k value for the given dataset.\n",
    "\n",
    "Algorithm for K-NN\n",
    "DistanceToNN=sort(distance from 1st example, distance from kth example)\n",
    "\n",
    "value i=1 to number of training records:\n",
    "\n",
    "Dist=distance(test example, ith example)\n",
    "\n",
    "if (Dist<any example in DistanceToNN):\n",
    "\n",
    "Remove the example from DistanceToNN and value.\n",
    "\n",
    "Put new example in DistanceToNN and value in sorted order.\n",
    "\n",
    "Return average of value\n",
    "\n",
    "Fit using K-NN is more reasonable than 1-NN, K-NN affects very less from noise if dataset is\n",
    "large.\n",
    "\n",
    "In K-NN algorithm, We can see jump in prediction values due to unit change in input. \n",
    "The reason for this due to change in neighbors. To handles this situation, We can use weighting \n",
    "of neighbors in algorithm. If the distance from neighbor is high, we want less effect from that \n",
    "neighbor. If distance is low, that neighbor should be more effective than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40708c5f-082d-4436-88c6-6e51c127d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol3...\n",
    "\n",
    "Knn Classifier: Predicts a class by using the highest majority category among its k nearest\n",
    "neighbors.\n",
    "\n",
    "Knn Regression: Predicts a value by using the mean of the k nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f06e8-b0c1-498c-aba9-e6c6c1382609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "Two common approaches are cross -validation and training/testing split. \n",
    "\n",
    "Overall, both cross -validation and training/testing split are useful approaches for\n",
    "evaluating the performance of the KNN algorithm, and the choice between them depends on the\n",
    "specific problem and the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae24792-2a0c-4490-99e1-ccac8b0f3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "The curse of dimensionality is a phenomenon that occurs in the k-nearest neighbors (kNN) \n",
    "algorithm when the distance between points increases as the number of dimensions increases.\n",
    "This can make it difficult to distinguish between candidate points in nearest neighbor \n",
    "calculations. \n",
    " \n",
    "#Here are some of the effects of the curse of dimensionality on kNN: \n",
    " \n",
    "1.Data points get farther apart. \n",
    " \n",
    "2.Assumptions of kNN classifiers are broken down. \n",
    " \n",
    "3.Increased computational efforts.\n",
    " \n",
    "4.Increased training data requirements\n",
    "\n",
    " \n",
    "The curse of dimensionality is also known as the Hughes Phenomenon. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58f1b1-16ad-4e8a-ba32-d3cdd3eff0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6...\n",
    "\n",
    "The KNNImputer works by finding the k-nearest neighbors (based on a specified distance metric)\n",
    "for the data points with missing values. It then imputes the missing values using the mean or \n",
    "median (depending on the specified strategy) of the neighboring data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4362a-f04a-4d89-b732-999a722e40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol7...\n",
    "\n",
    "The key differences are:\n",
    "\n",
    "KNN regression tries to predict the value of the output variable by using a local average.\n",
    "\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing\n",
    "the local probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e930e5-33e2-405e-afec-05bfcbc4b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol8...\n",
    "\n",
    "#Advantages of KNN:\n",
    "\n",
    "First, the algorithm is simple. Because it uses simple\n",
    "comparisons to find similar records in the training data, KNN is sometimes very effective at\n",
    "making good predictions.\n",
    "\n",
    "Second, KNN can be effective at capturing complex interactions among variables without having\n",
    "to define a separable statistical model (no coefficients and weights).\n",
    "\n",
    "#Disadvantages of KNN:\n",
    "A disadvantage of the KNN algorithm is that it does not create a generalized separable model.\n",
    "There is no summary equations or trees that can be produced by the training process that can be\n",
    "quickly applied to new records. \n",
    "Instead, KNN simply uses the training data itself to perform prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46692891-7617-4d3b-8725-f271405073b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol9...\n",
    "\n",
    "#The main difference between Euclidean distance and Manhattan distance:\n",
    " \n",
    "1.Euclidean distance:\n",
    "Measures the shortest distance between two points in a straight line. It's best for continuous\n",
    "data and open spaces. \n",
    " \n",
    "2.Manhattan distance\n",
    "Measures the distance by adding up the absolute differences between points across all dimensions.\n",
    "It's best for high-dimensional data and grid-based systems. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0e6e7-fdab-4fce-8ab9-9322a9d614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol10...\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
