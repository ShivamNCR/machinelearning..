{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d972033-426d-4fe1-8ab6-d3b693e37ded",
   "metadata": {},
   "source": [
    "# Questions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75086e41-563b-48c2-a7aa-d4e3396df3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "\n",
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?\n",
    "\n",
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?\n",
    "\n",
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?\n",
    "\n",
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?\n",
    "\n",
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?\n",
    "\n",
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f927a7-65d7-4311-aa40-c92e982d8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "\n",
    "A **contingency matrix** (also known as a **confusion matrix**) is a table used to evaluate the performance of a classification \n",
    "model by comparing the predicted labels against the true labels. \n",
    "It provides a summary of the number of true positives, false positives, true negatives, and false negatives:\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive instances.\n",
    "- **False Positives (FP)**: Incorrectly predicted as positive (but are negative).\n",
    "- **True Negatives (TN)**: Correctly predicted negative instances.\n",
    "- **False Negatives (FN)**: Incorrectly predicted as negative (but are positive).\n",
    "\n",
    "The matrix helps calculate other evaluation metrics like **accuracy**, **precision**, **recall**, and **F1 score**,\n",
    "allowing a deeper understanding of model performance beyond accuracy alone.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?\n",
    "\n",
    "A **pair confusion matrix** differs from a regular confusion matrix in that it is used for **pairwise classification** \n",
    "problems where pairs of objects are classified into categories. \n",
    "It is useful when the goal is to determine whether two items are similar or dissimilar (e.g., clustering, ranking tasks). \n",
    "\n",
    "The pair confusion matrix contains four categories: \n",
    "- **Correctly grouped pairs** (same class and correctly grouped)\n",
    "- **Incorrectly grouped pairs** (different classes, but incorrectly grouped)\n",
    "- **Correctly separated pairs** (different classes, correctly separated)\n",
    "- **Incorrectly separated pairs** (same class, but incorrectly separated)\n",
    "\n",
    "It is useful in evaluating **clustering algorithms** or **ranking systems**, where relationships between pairs of instances\n",
    "matter more than individual class labels.\n",
    "\n",
    "\n",
    "\n",
    "### Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?\n",
    "\n",
    "An **extrinsic measure** in NLP evaluates the performance of a model based on its ability to contribute to a downstream task. \n",
    "It measures how well the model performs when applied to a real-world task, such as:\n",
    "\n",
    "- Machine translation\n",
    "- Question answering\n",
    "- Sentiment analysis\n",
    "\n",
    "For example, a word embedding model might be evaluated on how well it improves the performance of a sentiment analysis task. \n",
    "**Extrinsic evaluation** helps assess the practical utility of the model in solving specific problems, and the quality of the \n",
    "embeddings, representations, or predictions in applied contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?\n",
    "\n",
    "An **intrinsic measure** in machine learning evaluates the model based on its internal quality, without applying it to\n",
    "a downstream task. It assesses the model based on charact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5557f-ad3e-4510-89ef-e882efd33920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "\n",
    "A **pair confusion matrix** differs from a regular confusion matrix in that it is used for **pairwise classification** \n",
    "problems where pairs of objects are classified into categories. \n",
    "It is useful when the goal is to determine whether two items are similar or dissimilar (e.g., clustering, ranking tasks). \n",
    "\n",
    "The pair confusion matrix contains four categories: \n",
    "- **Correctly grouped pairs** (same class and correctly grouped)\n",
    "- **Incorrectly grouped pairs** (different classes, but incorrectly grouped)\n",
    "- **Correctly separated pairs** (different classes, correctly separated)\n",
    "- **Incorrectly separated pairs** (same class, but incorrectly separated)\n",
    "\n",
    "It is useful in evaluating **clustering algorithms** or **ranking systems**, where relationships between pairs of instances\n",
    "matter more than individual class labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69dcddc-40b8-458e-9556-4bfdde7a7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol3...\n",
    "\n",
    "\n",
    "An **extrinsic measure** in NLP evaluates the performance of a model based on its ability to contribute to a downstream task. \n",
    "It measures how well the model performs when applied to a real-world task, such as:\n",
    "\n",
    "- Machine translation\n",
    "- Question answering\n",
    "- Sentiment analysis\n",
    "\n",
    "For example, a word embedding model might be evaluated on how well it improves the performance of a sentiment analysis task. \n",
    "**Extrinsic evaluation** helps assess the practical utility of the model in solving specific problems, and the quality of the \n",
    "embeddings, representations, or predictions in applied contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28c4a9-321e-4bc9-9f92-b861bae9f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "An **intrinsic measure** in machine learning evaluates the model based on its internal quality, without applying it to\n",
    "a downstream task. It assesses the model based on characteristics such as accuracy, loss, or similarity to known data.\n",
    "\n",
    "In NLP, intrinsic measures might include:\n",
    "\n",
    "- Perplexity (for language models)\n",
    "- BLEU score (for machine translation)\n",
    "- Word similarity (for word embeddings)\n",
    "\n",
    "The main difference from an **extrinsic measure** is that intrinsic measures assess the model performance on well-defined \n",
    "tasks or benchmarks, while extrinsic measures test how well the model contributes to a more complex task or system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053206e8-4a50-4665-9b07-57690f791a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "\n",
    "The **purpose of a confusion matrix** is to evaluate the performance of a classification model by showing the actual \n",
    "versus predicted outcomes. It provides insight into how well the model is making distinctions between classes by revealing \n",
    "areas where the model performs well and where it fails.\n",
    "\n",
    "The confusion matrix helps identify strengths and weaknesses, such as:\n",
    "- **High true positives and true negatives**: Indicating that the model is correctly classifying many instances.\n",
    "- **High false positives**: Showing that the model tends to incorrectly classify negatives as positives.\n",
    "- **High false negatives**: Showing that the model tends to miss positive instances.\n",
    "\n",
    "These insights allow developers to fine-tune models, optimize decision thresholds, or even balance the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b0af4-2045-46bc-8bfd-a7857a95beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6...\n",
    "\n",
    "\n",
    "\n",
    "Common intrinsic measures for **unsupervised learning algorithms** include:\n",
    "\n",
    "1. **Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters. A higher silhouette\n",
    "                         score (close to 1) indicates that instances are well clustered.\n",
    "                         \n",
    "2. **Inertia (Within-Cluster Sum of Squares)**: Measures the compactness of clusters. Lower inertia indicates \n",
    "                         tighter, well-defined clusters.\n",
    "                         \n",
    "3. **Davies-Bouldin Index**: Measures the average similarity ratio of each cluster to its most similar cluster. \n",
    "                         Lower values indicate better clustering.\n",
    "                         \n",
    "4. **Dunn Index**: Evaluates cluster compactness and separation. Higher values indicate well-separated and compact clusters.\n",
    "\n",
    "These metrics help assess the quality of clustering in terms of cohesion and separation without needing labeled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedc42c-9991-40ca-9fd8-8a9ee65cad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol7...\n",
    "\n",
    "\n",
    "\n",
    "The limitations of using **accuracy** as a sole evaluation metric include:\n",
    "\n",
    "1. **Class Imbalance**: In cases of highly imbalanced datasets, accuracy can be misleading because the model might simply \n",
    "                         predict the majority class, ignoring the minority class.\n",
    "                         \n",
    "2. **No Insight into Misclassification**: Accuracy doesn't indicate whether the errors are mostly false positives or false\n",
    "                         negatives, which may be critical in certain tasks (e.g., fraud detection, medical diagnosis).\n",
    "                         \n",
    "3. **Threshold Sensitivity**: Accuracy doesn’t account for the sensitivity of the decision threshold, which may require adjusting\n",
    "                             for better performance on certain metrics like precision or recall.\n",
    "\n",
    "These limitations can be addressed by using other metrics such as:\n",
    "- **Precision** and **Recall**: To understand the balance between false positives and false negatives.\n",
    "- **F1 Score**: A harmonic mean of precision and recall, helpful for imbalanced datasets.\n",
    "- **AUC-ROC**: Evaluates the model’s ability to distinguish between classes across thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65483db-0eda-4e6e-9c8f-25cfdd16690f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
