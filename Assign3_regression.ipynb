{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b5afe-35fa-4f9b-a1ae-8ac0e67f184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTIONS...\n",
    "\n",
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429161b-4f1a-4cf2-bc20-407569dbdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol1...\n",
    "\n",
    "R-squared:\n",
    "    R squared is calculation used to check or calculate the performance of a model based on Linear regression.\n",
    "    \n",
    "    \n",
    "    Mathematically, it can be expressed as:\n",
    "                   R-squared=1-(SS(residual)/SS(total))\n",
    "            \n",
    "SS(residual): represents sum of squares of errors from best fit line.\n",
    "SS(total):represents sum of squares of errors from average line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9440b03-261b-4327-8a76-96ca73cdcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "Adjusted R-squared():\n",
    "    Adjusted R-squared is a statistical measurement used to calculate the permormance and fittness of the model\n",
    "    based on the Linear regression.\n",
    "    \n",
    "#How it differs from R-Squared:\n",
    "Even if a new feature is correlated or not ,the R-squared value always incerases on adding irreevant feature.\n",
    "But it does not happen in case of Adjusted R-squared.It resolves the above problem.\n",
    "\n",
    "#Calculation:\n",
    "           \n",
    "    Adjusted R-squared=1-((1-R^2)(N-1))/(N-P-1)\n",
    "    \n",
    "R^2 reprsents R-squared value.\n",
    "N represents no. of data points.\n",
    "P represents no. of Independent features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69718b-cf89-44fe-be7e-66a7ea7d4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol3...\n",
    "\n",
    "R-squared, which always increases as you add more independent variables to the model (even if those variables\n",
    "are not truly related to the dependent variable), adjusted R-squared penalizes the addition of unnecessary\n",
    "variables.\n",
    "\n",
    "#Here's when adjusted R-squared is more appropriate:\n",
    "\n",
    "1.Comparing Models with Different Numbers of Variables.\n",
    "\n",
    "2.Preventing Overfitting.\n",
    "\n",
    "3.Large Number of Predictors.\n",
    "\n",
    "4.Model Selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e0f59-989a-436e-bc43-bd0afdefa2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "RMSE is a measure of the average magnitude of residuals (i.e., differences between predicted and actual values)\n",
    "in a regression model.\n",
    "It is calculated by taking the square root of the average of squared differences between predicted and actual \n",
    "values.\n",
    "The formula for RMSE is:\n",
    "             RMSE = sqrt [(Σ(Pi – Oi)²) / n]\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "MSE is similar to RMSE but without taking the square root. It represents the average squared difference between \n",
    "the predicted and actual values.\n",
    "\n",
    "The formula for MSE is:\n",
    "     MSE = Σ(yi − pi)2n\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "MAE is a measure of the average absolute magnitude of the residuals.\n",
    "It is calculated by taking the average of the absolute differences between predicted and actual values.\n",
    "The formula for MAE is:\n",
    "\n",
    "    MAE = (1/n) Σ(i=1 to n) |y_i – ŷ_i|\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf974ea-2d5f-45cb-81f5-5b3146432534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "1.It is Sensitive to large errors.\n",
    "2.The equation of MSE is Differentiable.\n",
    "3.It has only one local or global minima.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "1.It is not robust to outliers.\n",
    "2.It has Units inconsistency.\n",
    "\n",
    "Advantages of MAE:\n",
    "\n",
    "1.It is Sensitive to errors.\n",
    "2.It is robust to outliers.\n",
    "3.It has units consistency.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "1.In it, Convergance usually takes more time.\n",
    "\n",
    "Advantages of RMSE:\n",
    "\n",
    "1.It has Units consistency.\n",
    "2.The equation of MSE is Differentiable.\n",
    "3.It has only one local or global minima.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "    \n",
    "1.It is not robust to outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456c771-dcba-4fdc-b6cd-a2e2765ec9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6...\n",
    "\n",
    "Lasso Regularization:\n",
    "Lasso regularization is a technique used in linear regression and other regression models to prevent overfitting\n",
    "by adding a penalty term to the loss function.\n",
    "\n",
    "Formula:\n",
    "objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "Ridge Regression:\n",
    "Ridge regression—also known as L2 regularization—is one of several type. Regularization is a statistical method \n",
    "to reduce errors caused by overfitting on training data. Ridge regression specifically corrects for \n",
    "multicollinearity in regression analysis.\n",
    "\n",
    "Formula:\n",
    "Loss=MSE+λ∑β \n",
    "\n",
    "#When to use Lasso regularization:\n",
    "\n",
    "1.Feature selection: When dealing with high-dimensional data with many potentially irrelevant features, Lasso \n",
    "can automatically select important features by driving irrelevant coefficients to zero.\n",
    "\n",
    "2.Interpretability: If interpretability of the model is important and a simpler model with fewer features is \n",
    "preferred, Lasso can provide a more interpretable solution.\n",
    "\n",
    "3.Sparse solutions: When there is a need for a sparse solution with fewer nonzero coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eac754-f84c-4f3c-9f26-646d1b8f9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol7...\n",
    "\n",
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the standard \n",
    "loss function, which penalizes overly complex models with large coefficients. This penalty encourages the model\n",
    "to generalize better to unseen data by constraining the complexity of the model.\n",
    "\n",
    "#Here's how regularized linear models prevent overfitting:\n",
    "\n",
    "1.By Penalizing large coefficients.\n",
    "\n",
    "2.Encouraging simplicity.\n",
    "\n",
    "3.Feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6047536-c150-46e2-bd01-7c0cdbe47b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol8...\n",
    "\n",
    "#Here are some limitations:\n",
    "\n",
    "1.Loss of interpretability: Regularization techniques tend to shrink coefficients towards zero, which can lead to\n",
    "loss of interpretability.\n",
    "\n",
    "2.Model complexity selection: Regularization parameters (λ or alpha) need to be carefully chosen through \n",
    "hyperparameter tuning. Selecting the optimal value for the regularization parameter can be challenging and \n",
    "often requires cross-validation.\n",
    "\n",
    "3.Limited flexibility in capturing complex relationships: Regularized linear models assume a linear relationship\n",
    "between predictors and the target variable. While this assumption holds in many cases, there are scenarios\n",
    "where the relationship may be more complex and non-linear. \n",
    "\n",
    "4.Impact on predictive performance: Regularization techniques introduce bias into the model by penalizing large \n",
    "coefficients.\n",
    "\n",
    "\n",
    "5.Computational complexity: Regularized linear models may involve solving optimization problems with additional \n",
    "penalty terms, which can increase computational complexity compared to non-regularized models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa1e01-6485-4291-a977-ebe5ff2f3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol9...\n",
    "\n",
    "#Comparing the metrics:\n",
    "\n",
    "Model B (MAE of 8) has a lower error metric than Model A (RMSE of 10). In general, having a lower error metric\n",
    "indicates better performance, as it suggests that, on average, the model's predictions are closer to the actual \n",
    "values.\n",
    "Based on this comparison, Model B would be considered the better performer.\n",
    "\n",
    "#Limitations of the metrics:\n",
    "\n",
    "1.RMSE penalizes large errors more heavily due to the squaring operation, which can make it sensitive to outliers.\n",
    "\n",
    "2.MAE is less sensitive to outliers since it considers the absolute differences. However, it doesn't provide any \n",
    "information about the variability of the errors; all errors are treated equally.\n",
    "\n",
    "In summary, while Model B (with MAE of 8) is the better performer based on the provided metrics...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7cc3e-e6ea-42a5-adf5-1a1de2fc2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol10...\n",
    "\n",
    "#Comparing the models:\n",
    "\n",
    "1.The choice of the better performer depends on various factors such as the data characteristics, the importance\n",
    "of feature selection, and the interpretability of coefficients.\n",
    "\n",
    "2.Generally, if feature selection is crucial, Lasso regularization tends to perform well because it tends to \n",
    "shrink coefficients to zero, effectively performing variable selection. So, if Model B (Lasso) retains fewer\n",
    "features while maintaining good performance, it might be preferred, especially if interpretability is important.\n",
    "\n",
    "3.On the other hand, if interpretability is less important and multicollinearity is a concern, Ridge regularization\n",
    "might be preferred because it tends to shrink coefficients towards zero without necessarily setting them exactly\n",
    "to zero, thus reducing the impact of multicollinearity.\n",
    "\n",
    "4.The choice between Ridge and Lasso regularization also depends on the value of the regularization parameter.\n",
    "A smaller value of the regularization parameter typically leads to weaker regularization, potentially allowing \n",
    "the model to overfit more. Conversely, a larger value of the regularization parameter strengthens the \n",
    "regularization, potentially leading to underfitting. Therefore, the choice of the regularization parameter\n",
    "should also be considered carefully.\n",
    "\n",
    "#Trade-offs and limitations:\n",
    "\n",
    "1.Ridge regularization tends to shrink coefficients towards zero gradually, which can be beneficial for dealing \n",
    "with multicollinearity. However, it may not perform well if feature selection is essential.\n",
    "\n",
    "2.Lasso regularization can perform automatic feature selection by setting some coefficients exactly to zero.\n",
    "However, it may not handle multicollinearity as effectively as Ridge regularization.\n",
    "\n",
    "Both methods involve a trade-off between bias and variance. Increasing the regularization strength reduces variance but increases bias, and vice versa.\n",
    "Choosing the appropriate regularization method and parameter requires tuning and validation, as the optimal choice depends on the specific characteristics of the dataset and the goals of the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
