{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15bdfb66-acae-44da-a71e-121cd58dc3af",
   "metadata": {},
   "source": [
    "## Questions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e19bf4-678d-444c-9b4e-7dc6e36843f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3549b2-f217-404a-a964-3a4e8457b6bc",
   "metadata": {},
   "source": [
    "## Solutions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba3fce-3a1b-4af3-9221-f6a8921fae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol1...\n",
    "\n",
    "**Anomaly detection** is the process of identifying rare items, events, or observations that raise suspicions by differing significantly \n",
    "from the majority of the data. \n",
    "\n",
    "### Purpose:\n",
    "- **Identifying Outliers**: To find data points that do not conform to expected patterns, which may indicate errors, fraud, or rare events.\n",
    "    \n",
    "- **Improving Data Quality**: By detecting anomalies, organizations can clean data and ensure more accurate analysis.\n",
    "    \n",
    "- **Enhancing Security**: In cybersecurity, it helps detect unusual patterns that may indicate potential threats or attacks.\n",
    "    \n",
    "- **Monitoring Systems**: In industrial and operational contexts, it assists in identifying faults or failures in equipment or processes.\n",
    "\n",
    "Overall, anomaly detection is crucial for maintaining system integrity and ensuring the reliability of data-driven decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe7d66-3561-4c50-ab41-e8a3b1282906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "Anomaly detection faces several key challenges, including:\n",
    "\n",
    "1. **High Dimensionality**: In datasets with many features, anomalies may be difficult to identify due to the \"curse of dimensionality,\" where distance\n",
    "metrics become less meaningful.\n",
    "\n",
    "2. **Imbalanced Data**: Anomalies are often rare compared to normal instances, making it challenging to train models that can accurately identify them\n",
    "without being biased towards the majority class.\n",
    "\n",
    "3. **Dynamic Environments**: In real-time systems, the definition of what constitutes an anomaly may change over time due to evolving data patterns, \n",
    "requiring adaptive detection methods.\n",
    "\n",
    "4. **Noise and Outliers**: Noise in the data can lead to false positives (incorrectly identifying normal instances as anomalies) or false negatives \n",
    "    (failing to detect true anomalies).\n",
    "\n",
    "5. **Lack of Labeled Data**: Many anomaly detection methods require labeled datasets for supervised learning, which are often unavailable in practice.\n",
    "\n",
    "6. **Interpretability**: Understanding why a specific instance was classified as an anomaly can be challenging, especially in complex models, \n",
    "                        which may hinder trust and adoption in critical applications.\n",
    "\n",
    "7. **Scalability**: As datasets grow larger, the computational complexity of anomaly detection algorithms may lead to inefficiencies, making it \n",
    "                        difficult to scale to large volumes of data.\n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77004a-4b72-4d38-a38c-1b7262148e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sol3...\n",
    "\n",
    "#**Unsupervised Anomaly Detection** and **Supervised Anomaly Detection** differ primarily in their use of labeled data:\n",
    "\n",
    "### Unsupervised Anomaly Detection:\n",
    "- **No Labeled Data**: It operates on datasets without labeled instances, trying to identify anomalies based on patterns and statistical properties of\n",
    "    the data.\n",
    "- **Methods**: Techniques include clustering, density estimation, and distance-based methods (e.g., k-means, DBSCAN).\n",
    "- **Flexibility**: Useful when anomalies are rare or when labeled data is not available, but it may lead to higher false positive rates due to lack of \n",
    "    guidance.\n",
    "\n",
    "### Supervised Anomaly Detection:\n",
    "- **Labeled Data Required**: It uses labeled datasets where anomalies are explicitly marked, allowing the model to learn the characteristics of normal\n",
    "    and anomalous instances.\n",
    "- **Methods**: Techniques typically involve classification algorithms (e.g., decision trees, SVMs) that differentiate between normal and anomalous \n",
    "    classes.\n",
    "- **Accuracy**: Generally more accurate in identifying anomalies since the model is trained on both classes, but it requires extensive labeled data, \n",
    "    which may not always be feasible.\n",
    "\n",
    "In summary, unsupervised methods rely on inherent data patterns, while supervised methods utilize prior knowledge through labeled examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c42221-e3dc-4b8b-b000-495447b35bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "#Anomaly detection algorithms can be broadly categorized into the following main types:\n",
    "\n",
    "1. **Statistical Methods**:\n",
    "   - These methods model the distribution of normal data points and identify anomalies based on statistical properties \n",
    "     (e.g., Z-score, Gaussian distribution).\n",
    "   - Examples: Gaussian Mixture Models, Hypothesis Testing.\n",
    "\n",
    "2. **Machine Learning Methods**:\n",
    "   - **Supervised Learning**: Uses labeled datasets to train models that classify instances as normal or anomalous.\n",
    "     - Examples: Decision Trees, Support Vector Machines (SVMs).\n",
    "   - **Unsupervised Learning**: Operates on unlabeled data to find patterns or clusters and identifies anomalies based on distance or density.\n",
    "     - Examples: k-means, DBSCAN, Isolation Forest.\n",
    "\n",
    "3. **Ensemble Methods**:\n",
    "   - Combine multiple anomaly detection techniques to improve robustness and accuracy.\n",
    "   - Examples: Random Cut Forest, Bagging and Boosting approaches.\n",
    "\n",
    "4. **Deep Learning Methods**:\n",
    "   - Utilize neural networks to learn complex representations of data for anomaly detection.\n",
    "   - Examples: Autoencoders, Variational Autoencoders, Recurrent Neural Networks (RNNs).\n",
    "\n",
    "5. **Clustering-Based Methods**:\n",
    "   - Identify anomalies as points that are distant from any cluster or belong to small clusters.\n",
    "   - Examples: DBSCAN, K-means clustering.\n",
    "\n",
    "These categories encompass a range of approaches, allowing practitioners to choose methods based on the specific characteristics of their data and use\n",
    "                                                cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4f64e-1226-4439-8902-f7d867b53055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "#Distance-based anomaly detection methods generally make the following key assumptions:\n",
    "\n",
    "1. **Locality**: Anomalies are typically located in low-density regions compared to normal data points, meaning they are far from their \n",
    "    nearest neighbors.\n",
    "\n",
    "2. **Density Homogeneity**: The normal data points are expected to be distributed in a uniform density, while anomalies appear in regions of lower\n",
    "    density.\n",
    "\n",
    "3. **Metric Space**: A suitable distance metric (e.g., Euclidean, Manhattan) is assumed to effectively measure the similarity or dissimilarity between\n",
    "    data points.\n",
    "\n",
    "4. **Independence of Features**: It is often assumed that the features are independent or that their distribution is not strongly correlated, which\n",
    "    simplifies the modeling of distances.\n",
    "\n",
    "5. **Sparsity**: Anomalies are relatively rare compared to normal instances, which means most points in the dataset are expected to belong to the \n",
    "   normal class.\n",
    "\n",
    "These assumptions guide the design and effectiveness of distance-based anomaly detection techniques, such as k-nearest neighbors (KNN) and \n",
    "clustering-based methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942acc02-662b-4a4a-abac-8468454ace43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6....\n",
    "\n",
    "#The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the concept of local density. Hereâ€™s a brief overview of the process:\n",
    "\n",
    "1. **k-nearest Neighbors**: For each data point, the algorithm identifies its \\(k\\) nearest neighbors using a distance metric.\n",
    "\n",
    "2. **Local Density**: It calculates the local density of each point by considering the distances to its \\(k\\) nearest neighbors. This is often done\n",
    "   using a measure like the reachability distance.\n",
    "\n",
    "3. **LOF Score Calculation**:\n",
    "   - **Reachability Distance**: For a point \\(p\\), its reachability distance from a neighbor \\(o\\) is defined as the maximum of the distance from \\(p\\)\n",
    "     to its \\(k\\)-th nearest neighbor and the distance from \\(p\\) to \\(o\\).\n",
    "   - **Local Reachability Density (LRD)**: The LRD of a point is computed as the inverse of the average reachability distance of its \\(k\\) nearest \n",
    "      neighbors.\n",
    "   - **LOF Score**: The LOF score for a point is the ratio of its LRD to the average LRD of its \\(k\\) nearest neighbors. A higher LOF score indicates \n",
    "      that the point is an outlier.\n",
    "\n",
    "4. **Threshold**: Points with LOF scores significantly greater than 1 are considered anomalies, as they have a lower local density compared to their \n",
    "    neighbors.\n",
    "\n",
    "This approach allows LOF to identify outliers in varying density regions effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69661d7f-1842-4587-9379-00aa5ce5a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sol8...\n",
    "\n",
    "\n",
    "1. **Anomaly Score**:\n",
    "- Typically, in KNN-based anomaly detection, if a point has fewer neighbors than ( K ) within the defined radius, it can indicate that the point\n",
    "     is an outlier, especially if \\( K \\) is significantly larger than the local neighbor count.\n",
    "\n",
    "   - The exact anomaly score calculation might depend on the specific implementation, but a common approach is to compute the score as:\n",
    "     \n",
    "     {Anomaly Score} = ({K - {Number of Neighbors})/{K}\n",
    "     \n",
    "   - In this case:\n",
    "     \n",
    "     {Anomaly Score} = {10 - 2}{10} = 0.8\n",
    "     \n",
    "\n",
    "Thus, the anomaly score for the data point would be **0.8**, indicating a significant likelihood of being an anomaly since it has relatively few \n",
    "neighbors compared to \\( K \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a80d9-985e-4379-ac0e-bdd64d3ecc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol9...\n",
    "\n",
    "In the Isolation Forest algorithm, the anomaly score for a data point is calculated based on the average path length of the point in the isolation trees compared to the average path length of the trees in the forest. The formula for the anomaly score is given by:\n",
    "\n",
    "\n",
    "{Anomaly Score} = 2^{-E(h(x))/c(n)}\n",
    "\n",
    "\n",
    "Where:\n",
    "-E(h(x)) is the average path length of the data point x  in the trees.\n",
    "-c(n) is a constant that is derived from the average path length of a tree for a dataset of size  n , given by:\n",
    "\n",
    "\n",
    "c(n) = 2*ln(n - 1) + gamma\n",
    "\n",
    "\n",
    "Here, gamma is a constant (approximately 0.5772, the Euler-Mascheroni constant).\n",
    "\n",
    "### Given:\n",
    "- Number of data points ( n = 3000 )\n",
    "- Average path length of the data point ( E(h(x)) = 5.0 )\n",
    "\n",
    "### Step 1: Calculate ( c(n) )\n",
    "First, calculate  c(3000) :\n",
    "\n",
    "c(3000) = 2*ln(3000 - 1) + 0.5772\n",
    "\n",
    "### Step 2: Compute the Anomaly Score\n",
    "After calculating  c(n), substitute E(h(x)) and c(n)  into the anomaly score formula.\n",
    "\n",
    "#Let's calculate it step by step:\n",
    "\n",
    "### Calculated Results:\n",
    "- The value of  c(3000) is approximately **16.59**.\n",
    "- The anomaly score for the data point with an average path length of 5.0 is approximately **0.81**.\n",
    "\n",
    "This indicates that the data point has a relatively high likelihood of being an anomaly, as scores closer to 1 suggest that the point is more isolated from the rest of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
