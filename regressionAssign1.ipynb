{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5328d-4ceb-427a-81d3-7d7f090f9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questions...\n",
    "\n",
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bdd831-b30b-45f7-a2b0-f83458e1a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol1...\n",
    "\n",
    "#Simple linear regression:\n",
    "    \n",
    "When a  feature is simply dependent on another feature linearly,then we call it simple linear regression.\n",
    "\n",
    "Ex. we want to predict a student's exam score (dependent variable) based on the number of hours they study\n",
    "per week (independent variable).\n",
    "\n",
    "#Multiple linear regression:\n",
    "    \n",
    "when a feature is dependent on more than one feature ,then we call it as multiple linear regression.\n",
    "\n",
    "Ex.  consider a scenario where we want to predict a house's selling price (dependent variable) based on \n",
    "multiple factors such as square footage, number of bedrooms, and number of bathrooms (independent variables).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acc754-d2ef-4d9f-b85f-4c82c62ddee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "Linearity.\n",
    "\n",
    "1.Independence of Errors.\n",
    "\n",
    "2.Constant variance of errors.\n",
    "\n",
    "3.Normality of Errors.\n",
    "\n",
    "4.No Perfect Multicollinearity.\n",
    "                  \n",
    "#To check whether these assumptions hold in a given dataset, various diagnostic techniques and statistical tests:\n",
    "\n",
    "\n",
    "Residual Analysis: Plotting the residuals against the predicted values and independent variables can help \n",
    "identify patterns that violate assumptions such as linearity, independence of errors, and homoscedasticity.\n",
    "\n",
    "Normality Tests: Statistical tests, such as the Shapiro-Wilk test or visual inspection of a histogram or Q-Q plot \n",
    "of residuals, can assess whether the residuals are normally distributed.\n",
    "\n",
    "Heteroscedasticity Tests: Tests like the Breusch-Pagan test or White test can determine whether there is a \n",
    "systematic relationship between the variance of the residuals and the independent variables, indicating \n",
    "heteroscedasticity.\n",
    "\n",
    "Collinearity Diagnostics: Techniques like variance inflation factor (VIF) or correlation matrices can detect \n",
    "multicollinearity among independent variables.\n",
    "\n",
    "Cook's Distance: This measure helps identify influential data points that may have a significant impact on the \n",
    "regression model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000df5d-ee6c-4a57-9508-d687d5452942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol3...\n",
    "\n",
    "In a linear regression model of the form \n",
    "\n",
    "The slope : represents the change in the dependent variable (y) for a one-unit change in the independent variable (x).\n",
    "The intercept: represents the value of the dependent variable (y) when the independent variable (x) is zero.\n",
    "\n",
    "#Here's how you can interpret the slope and intercept in a real-world scenario:\n",
    "\n",
    "Example: Predicting House Prices\n",
    "Suppose you are analyzing a dataset of house prices based on their sizes (in square feet). You fit a simple linear regression model to predict \n",
    "house prices based on their sizes.\n",
    "\n",
    "Slope : Let's say the slope of the regression line is 100. This means that for every additional square foot in the size of the house, the \n",
    "predicted house price increases by $100, assuming all other factors remain constant. \n",
    "\n",
    "Intercept: Suppose the intercept of the regression line is $50,000. This means that if a house has zero square feet\n",
    "(which is an unrealistic scenario), the predicted price of that house would be $50,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d25db1-4379-4650-a973-fbc81b1dab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "Gradient descent:\n",
    "Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction\n",
    "of the steepest descent of the function.\n",
    "It's a fundamental technique in optimization and is widely used in machine learning.\n",
    "\n",
    "#Here's how gradient descent works:\n",
    "\n",
    "Initialization: Gradient descent starts by initializing the parameters or weights of the model with some arbitrary \n",
    "values.\n",
    "\n",
    "Compute Gradient:It then computes the gradient (or derivative) of the cost function with respect to each parameter.\n",
    "The gradient indicates the direction of the steepest increase of the function at the current point.\n",
    "\n",
    "Update Parameters: After computing the gradient, gradient descent updates the parameters in the direction\n",
    "opposite to the gradient. This means it adjusts the parameters to move towards the minimum of the cost function.\n",
    "\n",
    "Repeat: Steps 2 and 3 are repeated iteratively until a stopping criterion is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45924ba2-659f-464f-96d2-8354c1baae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "Differences:\n",
    "\n",
    "Number of Independent Variables: In simple linear regression, there is only one independent variable, \n",
    "whereas in multiple linear regression, there are two or more independent variables.\n",
    "\n",
    "Model Complexity: Multiple linear regression models are more complex than simple linear regression models \n",
    "because they account for the effects of multiple variables on the dependent variable.\n",
    "\n",
    "Interpretation of Coefficients: In simple linear regression, each coefficient represents the change in the \n",
    "dependent variable for a one-unit change in the independent variable.\n",
    "In multiple linear regression, the interpretation of coefficients becomes more nuanced because the effect of each \n",
    "independent variable is conditional on the values of the other independent variables.\n",
    "\n",
    "Assumptions: The assumptions underlying multiple linear regression are similar to those of simple linear regression\n",
    "but are extended to account for multiple predictors. These assumptions include linearity, independence of errors,\n",
    "homoscedasticity, normality of errors, and no perfect multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0bd3d-919a-4db4-9ea3-de59cc418b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6:\n",
    "\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more predictor variables in\n",
    "the model are highly correlated with each other. \n",
    "\n",
    "\n",
    "To detect multicollinearity, you can use various techniques:\n",
    "\n",
    "1.Correlation Matrix: Calculate the correlation coefficients between each pair of independent variables. \n",
    "If any correlation coefficients are close to +1 or -1, it indicates strong multicollinearity.\n",
    "\n",
    "2.Variance Inflation Factor (VIF): VIF quantifies the severity of multicollinearity in a regression analysis.\n",
    "It measures how much the variance of an estimated regression coefficient is increased because of collinearity.\n",
    "Generally, a VIF value exceeding 10 is considered problematic.\n",
    "\n",
    "3.Eigenvalues: Calculate the eigenvalues of the correlation matrix. If one or more eigenvalues are close to zero,\n",
    "it suggests multicollinearity.\n",
    "\n",
    "To address multicollinearity, several strategies can be employed:\n",
    "\n",
    "1. Feature Selection.\n",
    "2. Principal Component Analysis (PCA).\n",
    "3. Ridge Regression or Lasso Regression.\n",
    "4. Collect More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1f524-5866-4416-ac2c-32212dc923f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol 7...\n",
    "\n",
    "Polynomial regression is a type of regression analysis used to model the relationship between a dependent variable\n",
    "and one or more independent variables by fitting a polynomial equation to the data.\n",
    "While linear regression models assume a linear relationship between the dependent and independent variables, \n",
    "polynomial regression allows for more complex, nonlinear relationships to be captured.\n",
    "\n",
    "\n",
    "The main difference between polynomial regression and linear regression lies in the functional form of the \n",
    "relationship between the dependent and independent variables.\n",
    "Linear regression models assume a linear relationship, which means the response variable changes linearly\n",
    "with changes in the predictor variables. \n",
    "\n",
    "On the other hand, polynomial regression allows for nonlinear relationships, enabling more flexible modeling of \n",
    "complex data patterns. This makes polynomial regression suitable for cases where the relationship between \n",
    "variables cannot be adequately captured by a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421d44e-a124-4856-9fbb-4c058b9aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol 8...\n",
    "\n",
    "Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1.Capturing Nonlinear Relationships: Polynomial regression can model nonlinear relationships between the \n",
    "independent and dependent variables more accurately than linear regression. It can capture more complex \n",
    "patterns in the data, such as curves and bends.\n",
    "\n",
    "2. Flexibility: Polynomial regression provides more flexibility in fitting the data since it can accommodate\n",
    "higher-order polynomial terms. This flexibility allows the model to better adapt to the shape of the data.\n",
    "\n",
    "3.Improved Fit: In cases where the relationship between variables is nonlinear, polynomial regression can often \n",
    "provide a better fit to the data compared to linear regression. This can lead to more accurate predictions and \n",
    "interpretations.\n",
    "\n",
    "Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1. Overfitting: With higher-degree polynomial terms, there is a risk of overfitting the model to the training \n",
    "data. Overfitting occurs when the model captures noise or random fluctuations in the data, leading to poor \n",
    "generalization to unseen data.\n",
    "\n",
    "2. Interpretability: As the degree of the polynomial increases, the model becomes more complex, making it harder\n",
    "to interpret the relationships between variables. It may be challenging to discern meaningful patterns from the \n",
    "coefficients of high-order polynomial terms.\n",
    "\n",
    "3.Extrapolation: Extrapolating beyond the range of the observed data can be problematic in polynomial regression, \n",
    "especially with higher-degree polynomials. Extrapolation may lead to unreliable predictions since the model may\n",
    "not accurately capture the behavior of the data outside the observed range.\n",
    "\n",
    "In what situations would you prefer to use polynomial regression:\n",
    "\n",
    "1.Nonlinear Relationships: When the relationship between the dependent and independent variables is nonlinear, \n",
    "polynomial regression is a suitable choice. It can capture curved or non-monotonic relationships that cannot be\n",
    "adequately modeled by linear regression.\n",
    "\n",
    "2.Complex Data Patterns: Polynomial regression is useful when the data exhibit complex patterns that cannot be \n",
    "captured by simple linear models. \n",
    "\n",
    "3.Exploratory Analysis: Polynomial regression can be valuable in exploratory data analysis to understand the \n",
    "underlying relationships between variables. It allows for flexibility in modeling the data, enabling researchers\n",
    "to uncover hidden patterns or trends.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
