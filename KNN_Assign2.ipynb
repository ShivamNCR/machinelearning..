{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f2944-a131-4c6f-8d6b-5528881b3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTIONS...\n",
    "\n",
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques \n",
    "can be used to determine the optimal k value?\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor?\n",
    "In what situations might you choose one distance metric over the other?\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor?\n",
    "What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225db238-adcb-4bcd-8beb-93464626442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol1...\n",
    "\n",
    "The main difference between Euclidean distance and Manhattan distance in KNN lies in how they \n",
    "calculate the distance between two points:\n",
    "\n",
    "Euclidean distance measures the shortest straight-line distance (as the crow flies) between two\n",
    "points. It uses the formula:\n",
    "    D = ( x 2 − x 1 ) 2 + ( y 2 − y 1 ) 2\n",
    "    \n",
    "Manhattan distance measures the distance along axes at right angles (like navigating city blocks). \n",
    "It uses the formula:\n",
    "    D = |x1 - x2| + |y1 - y2|\n",
    "    \n",
    "Effect on KNN Performance:\n",
    "1.Euclidean distance tends to work better when the features have similar scales and the \n",
    "relationship between them is smooth and continuous.\n",
    "\n",
    "2.Manhattan distance can be more effective when dealing with high-dimensional or grid-like data\n",
    "(e.g., image or text data) where the directions matter more than the exact distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca947d4-b306-4979-8545-45fa082b5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "#you can use the following techniques:\n",
    "\n",
    "1.Cross-Validation: Split the dataset into training and validation sets, then evaluate different\n",
    "values of k. Choose the one that minimizes the error (e.g., accuracy for classification, mean \n",
    "squared error for regression) on the validation set.\n",
    "\n",
    "2.Elbow Method: Plot the error rate (for classification) or the mean squared error (for regression)\n",
    "against different k values. Look for the \"elbow\" point where the error stops decreasing \n",
    "significantly.\n",
    "\n",
    "3.Grid Search: Combine cross-validation with a systematic search through a range of k values, \n",
    "selecting the one with the best performance.\n",
    "\n",
    "4.Odd k for Classification: For classification tasks, it's recommended to use odd values of k to\n",
    "avoid ties between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee584f9-c168-4fb0-aa5a-2999c16b08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol3...\n",
    "\n",
    "The choice of distance metric in KNN directly impacts how the nearest neighbors are determined, \n",
    "affecting the performance of the classifier or regressor:\n",
    "\n",
    "#Euclidean Distance:\n",
    "\n",
    "1.Works well when the relationship between features is continuous and smooth.\n",
    "2.Suitable when all features have the same scale.\n",
    "3.Use when the data distribution is more spherical or when exact distances between points are\n",
    "meaningful.\n",
    "\n",
    "#Manhattan Distance:\n",
    "\n",
    "1.More appropriate for grid-like data (e.g., text or images) or when the features differ \n",
    "significantly in their ranges.\n",
    "\n",
    "2.Better when the data has a lot of outliers or follows a blocky pattern.\n",
    "\n",
    "3.Use when directions along axes matter more than exact proximity.\n",
    "\n",
    "#Minkowski Distance:\n",
    "\n",
    "1.A generalization of both Euclidean and Manhattan distance.\n",
    "\n",
    "2.Can be used when you want to experiment with a mix of these distance metrics by tuning the\n",
    "parameter.\n",
    "\n",
    "#Metric Choice Situations:\n",
    "Choose Euclidean when features are on a similar scale and the relationships between data points\n",
    "are smooth.\n",
    "Choose Manhattan when dealing with high-dimensional data, or when features are not on the same\n",
    "scale or the data follows a blocky structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3bfa0-4bc3-45c7-8805-ecc493698afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "#Common hyperparameters in KNN classifiers and regressors include:\n",
    "\n",
    "1.k (Number of Neighbors): Controls the number of nearest neighbors considered.\n",
    "\n",
    "Low k → More sensitive to noise, less smooth decision boundaries.\n",
    "High k → Smoother boundaries but may underfit.\n",
    "\n",
    "2.Distance Metric (e.g., Euclidean, Manhattan): Defines how distances between points are \n",
    "calculated.\n",
    "\n",
    "Affects neighbor selection and model accuracy based on data structure.\n",
    "\n",
    "3.Weighting: Neighbors can be weighted equally or by distance.\n",
    "\n",
    "Distance-weighted neighbors give closer points more influence.\n",
    "\n",
    "#Tuning:\n",
    "1.Grid Search or Random Search over a range of k values, distance metrics, and weights.\n",
    "2.Cross-Validation to select hyperparameters that minimize error on validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c328c-d4bb-4951-9e1b-ebf8c5e3116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "\n",
    "#The size of the training set affects KNN performance as follows:\n",
    "\n",
    "#Larger Training Set:\n",
    "\n",
    "1.Improves accuracy because more neighbors provide better predictions.\n",
    "2.Increases computational cost, making KNN slower for large datasets.\n",
    "\n",
    "#Smaller Training Set:\n",
    "\n",
    "1.Faster but can lead to underfitting and poor generalization.\n",
    "\n",
    "#Techniques to Optimize Training Set Size:\n",
    "    \n",
    "1.Cross-Validation: Test performance with different subsets of data to find the optimal size.\n",
    "\n",
    "2.Data Sampling: Use techniques like stratified sampling to maintain data distribution while\n",
    "reducing size.\n",
    "\n",
    "3.Dimensionality Reduction: Apply methods like PCA or feature selection to reduce dataset size \n",
    "without losing critical information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d206cd2-42c2-40f4-a5bf-95288bf17c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6...\n",
    "\n",
    "#Potential Drawbacks of KNN:\n",
    "\n",
    "1.Computationally Expensive: Slow for large datasets due to distance calculations for every query \n",
    "point.\n",
    "Solution: Use data structures like KD-trees or ball trees to speed up searches.\n",
    "\n",
    "2.Sensitive to Noise: KNN can be easily affected by outliers.\n",
    "Solution: Use robust distance metrics (e.g., Manhattan) or pre-process data to remove outliers.\n",
    "\n",
    "3.High Memory Usage: Requires storing all training data.\n",
    "Solution: Apply dimensionality reduction techniques (e.g., PCA).\n",
    "\n",
    "4.Feature Scaling: KNN is sensitive to features on different scales.\n",
    "Solution: Normalize or standardize data before applying KNN.\n",
    "\n",
    "5.Curse of Dimensionality: Performance degrades with high-dimensional data.\n",
    "Solution: Use feature selection or dimensionality reduction to limit the number of features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a86b0-6ff6-48b2-beba-98d4ffca7e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61519b2f-1d72-4fc9-b9eb-db53ad4ba006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
