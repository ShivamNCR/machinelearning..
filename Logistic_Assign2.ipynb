{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079093a4-a01f-4136-8810-355b1292411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTIONS...\n",
    "\n",
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.\n",
    "\n",
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "\n",
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "\n",
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?\n",
    "\n",
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?\n",
    "\n",
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?\n",
    "\n",
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cd650-8fab-4ab0-863e-929c8e11cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol1...\n",
    "\n",
    "Linear Regression:\n",
    "Purpose: Linear regression is used for predicting a continuous dependent variable based on one or more independent\n",
    "variables. The goal is to fit a linear relationship between the dependent and independent variables.\n",
    "\n",
    "Output: The output is a continuous value. For example, predicting house prices based on features like size, \n",
    "number of bedrooms, and location.\n",
    "\n",
    "\n",
    "Logistic Regression:\n",
    "Purpose: Logistic regression is used for predicting a binary outcome (a categorical dependent variable with two\n",
    "        possible outcomes) based on one or more independent variables. It estimates the probability that a given \n",
    "        input point belongs to a certain class.\n",
    "    \n",
    "Output: The output is a probability value between 0 and 1, which can be converted into a binary outcome\n",
    "(e.g., yes/no, success/failure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57391c95-bc9f-4c40-918f-83b20dd82406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol2...\n",
    "\n",
    "A cost function is a mathematical function that calculates the difference between the target actual values \n",
    "(ground truth) and the values predicted by the model. A function that assesses a machine learning model’s \n",
    "performance also referred to as a loss function or objective function.\n",
    "Usually, the objective of a machine learning algorithm is to reduce the error or output of cost function.\n",
    "\n",
    "When it comes to Linear Regression, the conventional Cost Function employed is the Mean Squared Error. \n",
    "The cost function (J) for m training samples can be written as:\n",
    "\n",
    " J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} [z^{(i)} - y^{(i)}]^{2}\n",
    "\n",
    "where,\n",
    "\n",
    "y^{(i)}   is the actual value of the target variable for the i-th training example.\n",
    "z^{(i)}=h_\\theta(x^{(i)})is the predicted value of the target variable for the i-th training example, calculated\n",
    "using the linear regression model with parameters θ.\n",
    "x^{(i)}   is the i-th training example.\n",
    "m is number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec65b05-48c5-4cd2-96bb-dce196ad1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol3...\n",
    "\n",
    "\n",
    "#Regularization...\n",
    "Regularization is a technique used in logistic regression (and other machine learning models) to prevent\n",
    "overfitting, which occurs when a model learns the noise in the training data rather than the actual underlying\n",
    "patterns.\n",
    "\n",
    "The commonly used regularization techniques are : \n",
    "\n",
    "Lasso Regularization – L1 Regularization.\n",
    "Ridge Regularization – L2 Regularization.\n",
    "Elastic Net Regularization – L1 and L2 Regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938feea-a0be-4720-8c5f-16f821ee66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol4...\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance\n",
    "of a binary classification model, such as logistic regression. It illustrates the trade-off between the true \n",
    "positive rate (sensitivity) and the false positive rate (1-specificity) at various threshold settings.\n",
    "\n",
    "Key Concepts\n",
    "True Positive Rate (TPR): Also known as sensitivity or recall, it is the proportion of actual positives that are\n",
    "correctly identified by the model.\n",
    "TPR=True Positives (TP)/True Positives (TP)+False Negatives \n",
    " \n",
    "\n",
    "False Positive Rate (FPR): It is the proportion of actual negatives that are incorrectly identified as positives\n",
    "by the model.\n",
    "FPR=False Positives (FP)/False Positives (FP)+True Negatives (TN)\n",
    "\n",
    "FPR= False Positives (FP)/True Negatives (TN)+False Positives (FP)\n",
    "\n",
    " \n",
    "\n",
    "The ROC Curve:\n",
    "X-Axis: The false positive rate (FPR).\n",
    "Y-Axis: The true positive rate (TPR).\n",
    "The ROC curve is plotted by varying the threshold for the classification decision. Each point on the ROC curve\n",
    "represents a TPR/FPR pair corresponding to a particular threshold.\n",
    "\n",
    "Steps to Create an ROC Curve\n",
    "Predict Probabilities: Use the logistic regression model to predict probabilities for the positive class.\n",
    "\n",
    "Vary Thresholds: Apply different threshold values to convert the predicted probabilities into binary\n",
    "classifications.\n",
    "\n",
    "Calculate TPR and FPR: For each threshold, calculate the TPR and FPR.\n",
    "\n",
    "Plot the Curve: Plot TPR against FPR for the different thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50471b4-b1ef-4b8c-9462-9eafcc5bbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol5...\n",
    "\n",
    "Feature selection is an important step in building a logistic regression model, as it helps in improving the\n",
    "model's performance by reducing overfitting, improving accuracy, and decreasing computational cost. \n",
    "\n",
    "#Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "1. Filter Methods\n",
    "Filter methods apply a statistical measure to assign a score to each feature. Features are ranked by their scores,\n",
    "and either selected or discarded.\n",
    "\n",
    "1.Chi-Square Test.\n",
    "2.ANOVA (Analysis of Variance).\n",
    "\n",
    "2. Wrapper Methods\n",
    "Wrapper methods use a predictive model to evaluate the combination of features and select the best subset. \n",
    "These methods can be computationally expensive but often provide better performance.\n",
    "\n",
    "1.Forward Selection.\n",
    "\n",
    "2.Backward Elimination.\n",
    "\n",
    "3.Recursive Feature Elimination (RFE).\n",
    "\n",
    "3. Embedded Methods\n",
    "Embedded methods perform feature selection during the model training process and are typically more efficient \n",
    "than wrapper methods.\n",
    "\n",
    "1.L1 Regularization (Lasso Regression).\n",
    "2.Principal Component Analysis (PCA).\n",
    "3.Linear Discriminant Analysis (LDA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c4a50-0d45-4c30-acdc-691ab7029c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol6...\n",
    "\n",
    "a. Oversampling the Minority Class.\n",
    "\n",
    "b.Random Oversampling: Randomly duplicates samples from the minority class.\n",
    "\n",
    "c. Undersampling the Majority Class.\n",
    "\n",
    "d. Algorithmic Approaches.\n",
    "\n",
    "d.Using Evaluation Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b07932-5fe3-4ef3-9c21-5f836d33cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol7...\n",
    "\n",
    "Implementing logistic regression can present several challenges and issues. Here are some common ones along with\n",
    "strategies to address them:\n",
    "\n",
    "1. Multicollinearity\n",
    "Issue: Multicollinearity occurs when two or more independent variables are highly correlated. This can make \n",
    "it difficult to determine the individual effect of each variable and can lead to unstable estimates of the\n",
    "regression coefficients.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Remove Highly Correlated Predictors: Identify and remove one of the correlated variables.\n",
    "\n",
    "Principal Component Analysis (PCA): Transform the variables into a set of linearly uncorrelated components.\n",
    "\n",
    "Regularization: Use L1 (Lasso) or L2 (Ridge) regularization, which can help manage multicollinearity by \n",
    "penalizing large coefficients.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
